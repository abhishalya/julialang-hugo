<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Beyond machine learning pipelines with MLJ</title>
<meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Official website for the Julia programming language. Join the Julia community today.">

<meta property="og:title" content="The Julia Language"/>
<meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/>
<meta property="og:description" content="Official website for the Julia programming language"/>

<link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
<link rel="stylesheet" href="/v2/css/bootstrap.min.css" />
<link rel="stylesheet" href="/v2/css/app.css" />
<link rel="stylesheet" href="/v2/css/fonts.css" />
<link rel="stylesheet" href="/v2/css/highlight/github.css" />

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
    onload="renderMathInElement(document.body, {
        delimiters: [
          {left: '$[[', right: ']]', display: true},
          {left: '\\[', right: '\\]', display: true},
          {left: '[[', right: ']]', display: false}
        ]
      });"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-28835595-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-28835595-1');
</script>

  

  
</head>

<body>
  
  

<div class="container py-3 py-lg-0">
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="main-menu">

    <a class="navbar-brand" href="/" id="logo">
      <img src="/v2/img/logo.svg" height="55" width="85" alt="JuliaLang Logo"/>
    </a>

    <button class="navbar-toggler ml-auto hidden-sm-up float-xs-left" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/downloads">Download</a>
        </li>
        <li class="nav-item flex-md-fill text-md-center">
          <a class="nav-link" href="//docs.julialang.org">Documentation</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/blog/">Blog</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/community">Community</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/learning">Learning</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/research">Research</a>
        </li>
        <li class="nav-item  flex-md-fill text-md-center">
          <a class="nav-link" href="/soc/ideas-page">JSoC</a>
        </li>
        <li class="nav-item donate flex-md-fill text-md-center">
          <a class="btn btn-success" href="https://numfocus.org/donate-to-julia">Donate</a>
        </li>
      </ul>
    </div>

  </nav>
</div>



  <br /><br />

  <div class="container">

  <div id="blogpost">
    <h1>Beyond machine learning pipelines with MLJ</h1>

    <p class="metadata">
      <span class="timestamp">02 May, 2019</span>
      
      &nbsp;|&nbsp;
      <span class="author">
        Anthony Blaom, Diego Arenas, Franz Kiraly, Yiannis Simillides, Sebastian Vollmer
      </span>
      
    </p>

    <!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="introduction">Introduction</h2>
<p><a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> is an
open-source machine learning toolbox written in pure Julia. It
provides a uniform interface for interacting with supervised and
unsupervised learning models currently scattered in different Julia
packages.</p>
<p>Building on a earlier proof-of-concept, development began in earnest
at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a> in
December 2018. In a short time interest grew and the project is now
the Institute's most starred software repository.</p>
<p>After outlining MLJ's current functionality, this post introduces MLJ
<strong>learning networks</strong>, a super-charged pipelining feature for model
composition.</p>
<p><strong>Quick links:</strong></p>
<p>☞ <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/frequently_asked_questions/">MLJ vs ScikitLearn.jl</a></p>
<p>☞ Video from <a href="https://www.youtube.com/watch?v=CfHkjNmj1eE">London Julia User Group meetup in March 2019</a> (skip to <a href="https://youtu.be/CfHkjNmj1eE?t=21m39s">demo at 21'39</a>)  </p>
<!-- raw HTML omitted -->
<p>☞  The MLJ <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/tour.ipynb">tour</a>
 </p>
<p>☞ Building a <a href="https://alan-turing-institute.github.io/MLJTutorials/">self-tuning random
forest</a>
 </p>
<p>☞ An MLJ <a href="https://github.com/ysimillides/mlj-docker">docker image</a> (including tour)</p>
<p>☞ Implementing the MLJ interface for a <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/">new model</a>
 </p>
<p>☞ How to <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/CONTRIBUTING.md">contribute</a></p>
<p>☞ Julia <a href="http://julialang.slack.com">Slack</a> channel: #mlj.</p>
<p>☞ Star'ing us to show support for <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> would be greatly appreciated!</p>
<h2 id="mlj-features">MLJ features</h2>
<p>MLJ already has substantial functionality:</p>
<ul>
<li>
<p><strong>Learning networks.</strong> Flexible model composition beyond traditional
pipelines (more on this below).</p>
</li>
<li>
<p><strong>Automatic tuning.</strong> Automated tuning of hyperparameters, including
composite models. Tuning implemented as a model wrapper for
composition with other meta-algorithms.</p>
</li>
<li>
<p><strong>Homogeneous model ensembling.</strong></p>
</li>
<li>
<p><strong>Registry for model metadata.</strong> Metadata available without loading
model code. Basis of a &ldquo;task&rdquo; interface and facilitates
model composition.</p>
</li>
<li>
<p><strong>Task interface.</strong> Automatically match models to specified learning
tasks, to streamline benchmarking and model selection.</p>
</li>
<li>
<p><strong>Clean probabilistic API.</strong> Improves support for Bayesian
statistics and probabilistic graphical models.</p>
</li>
<li>
<p><strong>Data container agnostic.</strong> Present and manipulate data in your
favorite Tables.jl format.</p>
</li>
<li>
<p><strong>Universal adoption of categorical data types.</strong> Enables model
implementations to properly account for classes seen in training but
not in evaluation.</p>
</li>
</ul>
<p>Enhancements planned for the near future include integration of
Flux.jl <strong>deep learning</strong> models, and <strong>gradient descent tuning</strong> of
continuous hyperparameters using automatic differentiation.</p>
<p>While a relatively small number of machine learning models currently
implement the MLJ interface, work in progress aims to wrap models
supported by the popular python framework, scikit-learn, as a
temporary expedient. For a comparison of the MLJ's design with the
Julia wrap <a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a>,
see this
<a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/frequently_asked_questions.md">FAQ</a>.</p>
<h2 id="learning-networks">Learning networks</h2>
<p>MLJ's model composition interface is flexible enough to implement, for
example, the <a href="https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html">model
stacks</a>
popular in data science competitions. To treat examples of this kind,
the interface design must account for the fact that information flow
in prediction and training modes is different. This can be seen from
the following schematic of a simple two-model stack, viewed as a
network:</p>
<p><img src="/images/blog/2019-05-02-MLJ/two_model_stack.png" alt=""></p>
<h2 id="building-a-simple-network">Building a simple network</h2>
<p>In MLJ, networks of models are built using a declarative syntax
already familiar from basic use of the package. For example, the
ordinary syntax for training a decision tree in MLJ, after one-hot
encoding the categorical features, looks like this:</p>
<pre><code class="language-julia">using MLJ
@load DecisionTreeRegressor

# load some data:
task = load_reduced_ames();
X, y = task();

# one-hot encode the inputs, X:
hot_model = OneHotEncoder()
hot = machine(hot_model, X)
fit!(hot)
Xt = transform(hot, X)

# fit a decision tree to the transformed data:
tree_model = DecisionTreeRegressor()
tree = machine(tree_model, Xt, y)
fit!(tree, rows = 1:1300)
</code></pre>
<p>Note that a <em>model</em> in MLJ is just a struct containing
hyperparameters. Wrapping a model in data delivers a <em>machine</em> struct,
which will additionally record the results of training.</p>
<p>Without a pipeline, each time we want to present new data for
prediction we must first apply one-hot encoding:</p>
<pre><code class="language-julia">Xnew = X[1301:1400,:];
Xnewt = transform(hot, Xnew);
yhat = predict(tree, Xnewt);
yhat[1:3]
 3-element Array{Float64,1}:
  223956.9999999999
  320142.85714285733
  161227.49999999994
</code></pre>
<p>To build a pipeline one simply wraps the supplied data in source nodes
and repeats similar declarations, omitting calls to
<code>fit!</code>. The difference now is that each &ldquo;variable&rdquo; (e.g., <code>Xt</code>,
<code>yhat</code>) is a node of our pipeline, instead of concrete data:</p>
<pre><code class="language-julia">Xs = source(X)
ys = source(y)

hot = machine(hot_model, Xs)
Xt = transform(hot, Xs);

tree = machine(tree_model, Xt, ys)
yhat = predict(tree, Xt)
</code></pre>
<p>If we like, we can think of a node as <em>dynamic data</em> - &ldquo;data&rdquo; because
it can be called (indexed) on rows, but &ldquo;dynamic&rdquo; because the result
depends on the outcome of training events, which in turn depend on
hyperparameter values. For example, after fitting the completed pipeline,
we can make new predictions like this:</p>
<pre><code class="language-julia">fit!(yhat, rows=1:1300)
 [ Info: Training NodalMachine @ 1…51.
 [ Info: Spawned 1300 sub-features to one-hot encode feature :Neighborhood.
 [ Info: Spawned 1300 sub-features to one-hot encode feature :MSSubClass.
 [ Info: Training NodalMachine @ 1…17.
 Node @ 1…79 = predict(1…17, transform(1…51, 1…07))

yhat(rows=1301:1302) # to predict on rows of source node
yhat(Xnew)           # to predict on new data
156-element Array{Float64,1}:
 223956.9999999999
 320142.85714285733
 ...
</code></pre>
<h2 id="exporting-and-retraining">Exporting and retraining</h2>
<p>Once a pipeline like this has been built and tested on sample data, it
can be exported as a stand-alone model, ready to be trained on any
dataset. For details, see the MLJ
<a href="https://alan-turing-institute.github.io/MLJTutorials/pub/getting-started/learning-networks.html">documentation</a>. In
the future, Julia macros will allow common architectures (e.g., linear
pipelines) to be built in a couple of lines.</p>
<p>Finally, we mention that MLJ learning networks, and their exported
counterparts, are &ldquo;smart&rdquo; in the sense that changing a hyperparameter
does not trigger retraining of component models upstream of the
change:</p>
<pre><code class="language-julia">tree_model.max_depth = 4
fit!(yhat, rows=1:1300)
 [ Info: Not retraining NodalMachine @ 1…51. It is up-to-date.
 [ Info: Updating NodalMachine @ 1…17.
 Node @ 1…79 = predict(1…17, transform(1…51, 1…07))
</code></pre>
<h2 id="just-write-the-math">Just &ldquo;Write the math!&rdquo;</h2>
<p>Because of Julia's generic programming features, any kind of operation
you would normally apply to data (arithmetic, row selection, column
concatenation, etc) can be overloaded to work with nodes. In this way,
MLJ's network-building syntax is economical, intuitive and easy to
read. In this respect we have been inspired by <a href="https://julialang.org/blog/2017/12/ml&amp;pl">On Machine Learning
and Programming Languages</a>.</p>
<h2 id="invitation-to-the-community">Invitation to the community</h2>
<p>We now invite the community to try out our newly registered <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> package and provide any feedback or suggestions you may have going forward. We are also particularly interested in hearing how you would use our package, and what features it may be lacking.</p>


  </div>

  

  </div>
  </div>
  </div>

  <br />

  


  <head>
  <meta name="description" content="We thank our contributors, donators, and Fastly for their support in keeping the Julia Language going. Donate here to help pay for Julia's needs."/>
</head>

<footer class="container-fluid footer-copy">
    <div class="container">
      <div class="row">
        <div class="col-md-10 py-2">
          <p>
            We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.
          </p>
          <p>
            ©2019 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>.
          </p>
        </div>
        <div class="col-md-2 py-2">
          <a class="btn btn-success" href="https://numfocus.org/donate-to-julia">Donate</a>
        </div>
      </div>
    </div>
</footer>


  <script src="/v2/js/jquery.min.js"></script>
<script src="/v2/js/bootstrap.min.js"></script>
<script src="/v2/js/platform.js"></script>
<script src="/v2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</body>

</html>
